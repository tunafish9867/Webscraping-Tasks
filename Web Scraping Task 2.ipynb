{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8fffc-b4c3-412d-8c35-7599acb50e7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 25 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 50 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 75 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 100 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 125 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 150 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 175 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 200 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 225 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 250 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 275 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 300 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 325 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 350 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 375 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 400 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 425 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 450 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 475 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 500 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 525 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 550 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 575 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 600 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 625 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 650 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 675 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 700 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 725 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 750 entries so far...\n",
      "Navigating to the next page...\n",
      "Extracted 774 entries so far...\n",
      "Timed out waiting for the 'Next' button.\n",
      "Total time elapsed: 87.31 seconds\n",
      "Data has been saved to C:/Users/denis/Downloads/Thesis/Pasay City Data_090224\\ordinances_1.xlsx\n"
     ]
    }
   ],
   "source": [
    "#This web scraping task involved extracting ordinance descriptions from the Pasay City ordinance website. The code used was specifically designed for this site.\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import os\n",
    "\n",
    "# Set up Chrome options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": \"C:/Users/denis/Downloads/Thesis/Pasay City Data_090224\",\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"safebrowsing.enabled\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# URL to scrape\n",
    "URL = 'https://www.pasaycitysecretariat.com/_Public/ordinancelist.aspx'\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Initialize lists to store extracted data\n",
    "numbers = []\n",
    "descriptions = []\n",
    "categories = []\n",
    "dates_enacted = []\n",
    "\n",
    "try:\n",
    "    # Generalized XPath to match rows with IDs like \"gvwList_DXDataRowX\"\n",
    "    rows_xpath = '//*[starts-with(@id, \"gvwList_DXDataRow\")]'\n",
    "\n",
    "    # Loop through pages to extract data\n",
    "    while True:\n",
    "        try:\n",
    "            # Find all rows matching the generalized XPath\n",
    "            rows = driver.find_elements(By.XPATH, rows_xpath)\n",
    "            if not rows:\n",
    "                print(\"No rows found.\")\n",
    "                break\n",
    "\n",
    "            # Extract data from each row\n",
    "            for row in rows:\n",
    "                try:\n",
    "                    # Extract each field from the corresponding <td> positions\n",
    "                    number = row.find_element(By.XPATH, './td[1]').text.strip()\n",
    "                    description = row.find_element(By.XPATH, './td[2]').text.strip()\n",
    "                    category = row.find_element(By.XPATH, './td[3]').text.strip()\n",
    "                    date_enacted = row.find_element(By.XPATH, './td[4]').text.strip()\n",
    "\n",
    "                    # Append extracted data to lists\n",
    "                    numbers.append(number)\n",
    "                    descriptions.append(description)\n",
    "                    categories.append(category)\n",
    "                    dates_enacted.append(date_enacted)\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Error finding data in one of the cells.\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"Extracted {len(numbers)} entries so far...\")\n",
    "\n",
    "            # Handle pagination: Click the \"Next\" button if available\n",
    "            next_button_xpath = '//img[@class=\"dxWeb_pNext_Office2010Black\" and @alt=\"Next\"]'\n",
    "            try:\n",
    "                # Locate the image element\n",
    "                next_button_img = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, next_button_xpath))\n",
    "                )\n",
    "\n",
    "                # Find the parent element (usually an <a> or <button>) and click it\n",
    "                parent_button = next_button_img.find_element(By.XPATH, './ancestor::a[1]')\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable(parent_button)\n",
    "                ).click()\n",
    "\n",
    "                print(\"Navigating to the next page...\")\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.staleness_of(next_button_img)\n",
    "                )\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(\"Timed out waiting for the 'Next' button.\")\n",
    "                break\n",
    "            except StaleElementReferenceException:\n",
    "                print(\"Stale element reference. The button may have changed.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error clicking the next button: {e}\")\n",
    "                break\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for elements.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while extracting data: {e}\")\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "    # Stop the timer and calculate the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Create a DataFrame and save it to an Excel file\n",
    "    df_main = pd.DataFrame({\n",
    "        'Number': numbers,\n",
    "        'Ordinance Description': descriptions,\n",
    "        'Category': categories,\n",
    "        'Date Enacted': dates_enacted\n",
    "    })\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    output_directory = \"C:/Users/denis/Downloads/Thesis/Pasay City Data_090224\"\n",
    "    output_file = os.path.join(output_directory, 'ordinances_1.xlsx')\n",
    "\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        df_main.to_excel(writer, sheet_name='Ordinances', index=False)\n",
    "\n",
    "    print(f\"Data has been saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
